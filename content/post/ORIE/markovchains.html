---
title: "Basics of Markov Chains"
author: "Erick Jones"
date: 2020-01-04
categories: ["ORIE Basics"]
tags: ["R Markdown", "Stochastic Processes", "Markov Chains", "Statistical Testing"]
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>This post explores how to markov chains work and how to visulaize them in R.
I use a R package specifically designed to visualize markov chains.
I also represent these markov chains using tables.
This is a reproducible example if you have R Studio just make sure you have installed the correct packages.</p>
<pre class="r"><code>library(markovchain)</code></pre>
<pre><code>## Warning: package &#39;markovchain&#39; was built under R version 4.0.2</code></pre>
<pre class="r"><code>library(diagram)
#Allows the use of exponential operators in matrix
library(expm)</code></pre>
<pre><code>## Warning: package &#39;expm&#39; was built under R version 4.0.2</code></pre>
<pre class="r"><code>#library(matlib)</code></pre>
<pre class="r"><code># A good article about Markov Chain Monte Carlo Methods https://towardsdatascience.com/a-zero-math-introduction-to-markov-chain-monte-carlo-methods-dcba889e0c50

# A Simple Example from https://www.analyticsvidhya.com/blog/2014/07/markov-chain-simplified/
# Creating a transition matrix
trans_mat &lt;- matrix(c(0.7,0.3,0.1,0.9),nrow = 2, byrow = TRUE)
trans_mat</code></pre>
<pre><code>##      [,1] [,2]
## [1,]  0.7  0.3
## [2,]  0.1  0.9</code></pre>
<pre class="r"><code># create the Discrete Time Markov Chain
disc_trans &lt;- new(&quot;markovchain&quot;,transitionMatrix=trans_mat, states=c(&quot;Pepsi&quot;,&quot;Coke&quot;), name=&quot;MC 1&quot;) 
mcDF &lt;- as(disc_trans,&quot;data.frame&quot;)
mcDF</code></pre>
<pre><code>##      t0    t1 prob
## 1 Pepsi Pepsi  0.7
## 2 Pepsi  Coke  0.3
## 3  Coke Pepsi  0.1
## 4  Coke  Coke  0.9</code></pre>
<pre class="r"><code>disc_trans</code></pre>
<pre><code>## MC 1 
##  A  2 - dimensional discrete Markov Chain defined by the following states: 
##  Pepsi, Coke 
##  The transition matrix  (by rows)  is defined as follows: 
##       Pepsi Coke
## Pepsi   0.7  0.3
## Coke    0.1  0.9</code></pre>
<pre class="r"><code>plot(disc_trans)</code></pre>
<p><img src="/post/ORIE/markovchains_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>#Market Share after one month
Current_state &lt;- c(0.55,0.45)
steps &lt;- 1
finalState &lt;- Current_state*disc_trans^steps #using power operator
finalState</code></pre>
<pre><code>##      Pepsi Coke
## [1,]  0.43 0.57</code></pre>
<pre class="r"><code>#Market Share after two month
Current_state &lt;- c(0.55,0.45)
steps &lt;- 2
finalState &lt;- Current_state*disc_trans^steps #using power operator
finalState</code></pre>
<pre><code>##      Pepsi  Coke
## [1,] 0.358 0.642</code></pre>
<pre class="r"><code>#Markov Chain Statistical Operations
steadyStates(disc_trans)</code></pre>
<pre><code>##      Pepsi Coke
## [1,]  0.25 0.75</code></pre>
<pre class="r"><code>meanFirstPassageTime(disc_trans)</code></pre>
<pre><code>##       Pepsi     Coke
## Pepsi     0 3.333333
## Coke     10 0.000000</code></pre>
<pre class="r"><code>meanRecurrenceTime(disc_trans)</code></pre>
<pre><code>##    Pepsi     Coke 
## 4.000000 1.333333</code></pre>
<pre class="r"><code>hittingProbabilities(disc_trans)</code></pre>
<pre><code>##       Pepsi Coke
## Pepsi     1    1
## Coke      1    1</code></pre>
<pre class="r"><code>meanAbsorptionTime(disc_trans)</code></pre>
<pre><code>## named numeric(0)</code></pre>
<pre class="r"><code>#absorptionProbabilities(disc_trans)
period(disc_trans)</code></pre>
<pre><code>## [1] 1</code></pre>
<pre class="r"><code>summary(disc_trans)</code></pre>
<pre><code>## MC 1  Markov chain that is composed by: 
## Closed classes: 
## Pepsi Coke 
## Recurrent classes: 
## {Pepsi,Coke}
## Transient classes: 
## NONE 
## The Markov chain is irreducible 
## The absorbing states are: NONE</code></pre>
<pre class="r"><code>#Manually Calculating Markov Chains
#https://www.probabilitycourse.com/chapter11/11_2_1_introduction.php

#Chapman-Kolmogorov Equation P^(n) = P^n
#p_ij^(m+n) = P(X_m+n = j | X_0 = i) = sum(p_ik^(m)*p_kj^(n))

#Probabilty Space after 5 steps
steps &lt;- 5

Current_state%*%(trans_mat%^%steps)</code></pre>
<pre><code>##          [,1]     [,2]
## [1,] 0.273328 0.726672</code></pre>
<pre class="r"><code>#Mean Return and Mean Hitting Times using Recursive Equations
#r_l = 1 + sum(t_k*p_lk)
#t_l = 0; t_k = 1 + sum(t_j*p_kj)

#Given X_0 = Coke time until pepsi first time, t_pepsi = 0
# t_coke = 1 + 1/10*t_pepsi + 9/10t_coke
t_coke &lt;- solve(1/10,1)
#r_pepsi = 1 + 7/10*t_pepsi + 3/10*t_coke
r_pepsi &lt;- 1 + 3/10*t_coke

meanFirstPassageTime(disc_trans)</code></pre>
<pre><code>##       Pepsi     Coke
## Pepsi     0 3.333333
## Coke     10 0.000000</code></pre>
<pre class="r"><code>t_coke</code></pre>
<pre><code>## [1] 10</code></pre>
<pre class="r"><code>meanRecurrenceTime(disc_trans)</code></pre>
<pre><code>##    Pepsi     Coke 
## 4.000000 1.333333</code></pre>
<pre class="r"><code>r_pepsi</code></pre>
<pre><code>## [1] 4</code></pre>
<pre class="r"><code>#Steady State
#Stationary Distribtution pi = pi*P, sum(pi) = 1 and if irreducible and aperiodic pi_j = lim(n&gt;inf)P(X_n =j | X_0 = i)
#pi_p = 7/10pi_p+1/10pi_c; pi_c = 3/10pi_p + 9/10pi_c, pi_c+pi_p =1
A &lt;- matrix(c(-3/10,1/10,3/10,-1/10,1,1), nrow =3, byrow = TRUE )
B &lt;- c(0,0,1)
steadyStates(disc_trans)</code></pre>
<pre><code>##      Pepsi Coke
## [1,]  0.25 0.75</code></pre>
<pre class="r"><code># Solve(A,B)




#rm(Current_state, disc_trans, finalState,steps,trans_mat)</code></pre>
<pre class="r"><code>#Continous Time Markov Chains

energyStates &lt;- c(&quot;sigma&quot;, &quot;sigma_star&quot;)
#Must produce generator matrix from a transistion probablity matrix
Q &lt;- expm::logm(disc_trans@transitionMatrix,method=&#39;Eigen&#39;)

gen &lt;- matrix(data = c(-3, 3, 1, -1), nrow = 2, byrow = TRUE, dimnames = list(energyStates, energyStates))

molecularCTMC &lt;- new(&quot;ctmc&quot;, states = energyStates, byrow = TRUE, generator = gen, name = &quot;Molecular Transition Model&quot;)

statesDist &lt;- c(0.8, 0.2)
rctmc(n = 3, ctmc = molecularCTMC, initDist = statesDist, out.type = &quot;df&quot;, include.T0 = FALSE, T = 4)</code></pre>
<pre><code>##       states              time
## 1      sigma 0.490779113024473
## 2 sigma_star 0.893907884742721
## 3      sigma  1.83824493102602</code></pre>
<pre class="r"><code>steadyStates(molecularCTMC)</code></pre>
<pre><code>##      sigma sigma_star
## [1,]  0.25       0.75</code></pre>
<pre class="r"><code>#Q-Learning with Liars Dice
#http://gradientdescending.com/q-learning-example-with-liars-dice-in-r/

# play a round of liars dice
liars.dice.round &lt;- function(players, control, player.dice.count, agents, game.states, reward, Q.mat, a = 1, verbose = 1){
  
  # set array for recording results
  y.ctrl = c(); y.state = c(); y.action = c()
  
  # roll the dice for each player
  if(verbose &gt; 0) cat(&quot;\n\n&quot;)
  rolls &lt;- lapply(1:players, function(x) sort(sample(1:6, player.dice.count[[x]], replace = TRUE)))
  if(verbose &gt; 1) lapply(rolls, function(x) cat(&quot;dice: &quot;, x, &quot;\n&quot;))
  total.dice &lt;- sum(unlist(player.dice.count))
  
  # set penalty
  penalty &lt;- sapply(1:players, function(x) 0, simplify = FALSE)
  
  # print dice blocks
  if(verbose &gt; 0) Dice(rolls[[1]])
  
  # set up roll table
  roll.table &lt;- roll.table.fn(rolls)
  
  # initial bid
  if(verbose &gt; 0) cat(&quot;place first bid\nPlayer&quot;, control, &quot;has control\n&quot;)
  if(control == a){
    
    dice.value &lt;- set.dice.value(&quot;dice value: &quot;, 6)
    dice.quantity &lt;- set.dice.value(&quot;quantity; &quot;, sum(roll.table))
    
  }else{
    
    # agent plays
    p1.state &lt;- which(game.states$total == total.dice &amp; game.states$p1 == player.dice.count[[1]] &amp; game.states$prob_cat == total.dice)
    pars &lt;- list(dice = rolls[[control]], total.dice = total.dice, dice.value = NULL, dice.quantity = 0, p1.state = p1.state)
    agent.action &lt;- agents[[control]](pars = pars, Q.mat = Q.mat)
    dice.value &lt;- agent.action$dice.value
    dice.quantity &lt;- agent.action$dice.quantity
    
  }
  
  
  # calculate probability cat and determine the game state
  # action set to raise because you can&#39;t call without an initial bid
  # this could be a 3rd action (initial bid) but it&#39;s not really necessary
  player.dice.qty &lt;- table(rolls[[1]])[as.character(dice.value)]
  player.dice.qty &lt;- ifelse(is.na(player.dice.qty), 0, player.dice.qty) %&gt;% unname
  prob.cat &lt;- calc.prob(c(total.dice, player.dice.count[[1]], dice.quantity, player.dice.qty))
  p1.state &lt;- which(game.states$total == total.dice &amp; game.states$p1 == player.dice.count[[1]] &amp; game.states$prob_cat == prob.cat)
  p1.action &lt;- &quot;raise&quot;
  
  # storing states for Q iteration
  y.ctrl = c(); y.state = c(); y.action = c()
  
  # moving control to the next player
  # storing the previous player since if the next player calls the previous player could lose a die
  prev &lt;- control
  control &lt;- control %% players + 1
  if(verbose &gt; 0) cat(&quot;dice value &quot;, dice.value, &quot;; dice quantity &quot;, dice.quantity, &quot;\n&quot;)
  
  
  # loop through each player and continue until there is a winner and loser
  called &lt;- FALSE
  while(!called){
    
    # check if the player with control is still in the game - if not skip
    if(player.dice.count[[control]] &gt; 0){
      if(control == a){
        
        action &lt;- readline(&quot;raise or call (r/c)? &quot;)
        
      }else{
        
        # the agent makes a decision
        pars &lt;- list(dice = rolls[[control]], total.dice = total.dice, dice.value = dice.value, dice.quantity = dice.quantity, p1.state = p1.state)
        agent.action &lt;- agents[[control]](pars = pars, Q.mat = Q.mat)
        action &lt;- agent.action$action
        
      }
      
      
      # storing states for reward iteration
      if(control == 1 &amp; !is.null(agent.action$action)){
        player.dice.qty &lt;- table(rolls[[1]])[as.character(dice.value)]
        player.dice.qty &lt;- ifelse(is.na(player.dice.qty), 0, player.dice.qty) %&gt;% unname
        
        p1.action &lt;- agent.action$action
        prob.cat &lt;- calc.prob(c(total.dice, player.dice.count[[1]], dice.quantity, player.dice.qty))
        p1.state &lt;- which(game.states$total == total.dice &amp; game.states$p1 == player.dice.count[[1]] &amp; game.states$prob_cat == prob.cat)
      }
      
      
      # called
      if(action %in% c(&quot;call&quot;, &quot;c&quot;)){
        
        if(verbose &gt; 0) {
          cat(&quot;player&quot;, control, &quot;called\nRoll table\n&quot;)
          print(roll.table)
        }
        
        # dice are reavealed
        
        # check if the quantity of dice value is less or more than the total in the pool
        # if more control loses otherwise control-1 win
        if(dice.quantity &gt; roll.table[dice.value]){
          
          penalty[[prev]] &lt;- penalty[[prev]] - 1
          if(verbose &gt; 0) cat(&quot;player&quot;, prev, &quot;lost a die\n&quot;)
          
        }else{
          
          penalty[[control]] &lt;- penalty[[control]] - 1
          if(verbose &gt; 0) cat(&quot;player&quot;, control, &quot;lost a die\n&quot;)
          
        }
        
        # for Q iteration
        y.ctrl &lt;- c(y.ctrl, control); y.state &lt;- c(y.state, p1.state); y.action &lt;- c(y.action, p1.action)
        
        # if called use the penalty array to change states
        prob.cat &lt;- calc.prob(c(total.dice, player.dice.count[[1]], dice.quantity, player.dice.qty))
        p1.state &lt;- which(game.states$total == total.dice-1 &amp; game.states$p1 == player.dice.count[[1]]+penalty[[1]] &amp; game.states$prob_cat == prob.cat)
        
        # break the loop
        called &lt;- TRUE
        
      }else{
        
        if(verbose &gt; 0) cat(&quot;player&quot;, control, &quot;raised\n&quot;)
        
        if(control == a){
          
          # player sets next dice value
          dice.value &lt;- set.dice.value(&quot;dice value: &quot;, 6)
          dice.quantity &lt;- set.dice.value(&quot;quantity; &quot;, sum(roll.table))
          
        }else{
          
          dice.value &lt;- agent.action$dice.value
          dice.quantity &lt;- agent.action$dice.quantity
        }
        
        # p1 state after the raise
        prob.cat &lt;- calc.prob(c(total.dice, player.dice.count[[1]], dice.quantity, player.dice.qty))
        p1.state &lt;- which(game.states$total == total.dice &amp; game.states$p1 == player.dice.count[[1]] &amp; game.states$prob_cat == prob.cat)
        if(verbose &gt; 0) cat(&quot;dice value&quot;, dice.value, &quot;; dice quantity&quot;, dice.quantity, &quot;\n&quot;)
      }
      
      # store info for Q update
      y.ctrl &lt;- c(y.ctrl, control); y.state &lt;- c(y.state, p1.state); y.action &lt;- c(y.action, p1.action)
      
      # set the control player to now be the previous player
      prev &lt;- control
    }
    
    # next player has control
    control &lt;- control %% players + 1
  }
  
  # play results and return
  play &lt;- data.frame(y.ctrl, y.state, y.action)
  return(list(penalty = penalty, play = play))
}








# play a full game of liars dice
play.liars.dice &lt;- function(players = 4, num.dice = 6, auto = FALSE, verbose = 1, agents, Q.mat = NULL, train = FALSE, print.trans = FALSE){
  
  # begin!
  if(verbose &gt; 0) liars.dice.title()
  
  # setting the number of dice each player has
  ndice &lt;- sapply(rep(num.dice, players), function(x) x, simplify = FALSE)
  players.left &lt;- sum(unlist(ndice) &gt; 0)
  
  # setting game states matrix
  game.states &lt;- generate.game.states(players, num.dice)
  
  # set up reward matrix
  reward &lt;- generate.reward.matrix(game.states)
  reward &lt;- list(raise = reward, call = reward)
  
  # set Q matrix if null
  if(is.null(Q.mat)) Q.mat &lt;- matrix(0, nrow = nrow(reward$raise), ncol = length(reward), dimnames = list(c(), names(reward)))
  
  # while there is at least 2 left in the game
  # who has control
  ctrl &lt;- sample(1:players, 1)
  play.df &lt;- data.frame()
  while(players.left &gt; 1){
    
    # play a round
    results &lt;- liars.dice.round(
      players = players, 
      control = ctrl,
      player.dice.count = ndice, 
      game.states = game.states,
      reward = reward,
      Q.mat = Q.mat,
      agents = agents,
      a = as.numeric(!auto),
      verbose = verbose
    )
    
    # update how many dice the players are left with given the 
    # outcomes of the round
    for(k in seq_along(ndice)){
      ndice[[k]] &lt;- ndice[[k]] + results$penalty[[k]]
      if(ndice[[k]] == 0 &amp; results$penalty[[k]] == -1){
        if(verbose &gt; 0) cat(&quot;player&quot;, k, &quot;is out of the game\n&quot;)
      }
      
      # update who has control so they can start the bidding
      if(results$penalty[[k]] == -1){
        ctrl &lt;- k
        while(ndice[[ctrl]] == 0){
          ctrl &lt;- ctrl %% players + 1
        }
      }
    }
    
    # checking how many are left and if anyone won the game
    players.left &lt;- sum(unlist(ndice) &gt; 0)
    if(players.left == 1){
      if(verbose &gt; 0) cat(&quot;player&quot;, which(unlist(ndice) &gt; 0), &quot;won the game\n&quot;)
    }
    
    # appending play
    play.df &lt;- rbind(play.df, results$play)
  }
  
  if(print.trans) print(play.df)
  
  # update Q
  # rather than training after each action, training at the 
  # end of each game in bulk
  # just easier this way
  if(train) Q.mat &lt;- update.Q(play.df, Q.mat, reward)
  
  # return the winner and Q matrix
  return(list(winner = which(unlist(ndice) &gt; 0), Q.mat = Q.mat))
}</code></pre>
<pre class="r"><code>#Other Stochastic Processes
#Martingales http://gradientdescending.com/martingale-strategies-dont-work-but-we-knew-that-simulation-analysis-in-r/
#https://github.com/doehm/martingale

#Bayesian Networks http://gradientdescending.com/simulating-data-with-bayesian-networks/

#Other Q Learning https://www.r-bloggers.com/a-simple-intro-to-q-learning-in-r-floor-plan-navigation/
#https://dataaspirant.com/2018/02/05/reinforcement-learning-r/</code></pre>
<p>Add a new chunk by clicking the <em>Insert Chunk</em> button on the toolbar or by pressing <em>Ctrl+Alt+I</em>.</p>
<p>When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the <em>Preview</em> button or press <em>Ctrl+Shift+K</em> to preview the HTML file).</p>
<p>The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike <em>Knit</em>, <em>Preview</em> does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.</p>
